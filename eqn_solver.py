# This script will use the Boltzmann equation data that is generated by the data generator script, and train a scikit-learn classifier. The classifier will be able to know whether two parameters will lead to a physically sound asymmetry or not.

import pandas as pd
import numpy as np
import time
from sklearn.ensemble import RandomForestClassifier
from sklearn import cross_validation
from sklearn.metrics import log_loss
from sklearn.metrics import mean_squared_error
from sklearn.utils import shuffle		# allows to shuffle rows of the dataframes

def data_import(data = 'Asymm_data.csv'):
	return pd.read_csv(data, delimiter=',', header=0, quoting=0)

def train_test_data_split(R):	# R is the ratio of train data compared to the full set
	df = data_import()		# Use df.iloc[i,j] to return specific values
	df = pd.DataFrame(shuffle(df)[:,1:],columns = ['eps','K','Ylf','result','result binary'])	#The output of the shuffling of the dataframe is a list of lists. The first element of each list is the ID of the initial DataFrame. This ID is not part of the data, but merely an index. In order to remove the ID from the shuffled list and to recreate a randomized DataFrame, we have to create a DataFrame out of the elements [:,1:] of the list.
	N_train = int(R*len(df))
	N_test = len(df)-N_train
	train_data = df[:N_train]
	test_data = df[N_train:]
	return train_data, test_data, df

df_train, df_test, df = train_test_data_split(0.5)


WtoC = len(df[df['result binary'] == 1])*len(df)**(-1)
WtoC_train = len(df_train[df_train['result binary'] == 1])*len(df_train)**(-1)
WtoC_test = len(df_test[df_test['result binary'] == 1])*len(df_test)**(-1)

# The WtoC (Wrong to Correct) ratio determines the amount of wrong values compared to the correct values in the full data set. The ratio turns out to be 96%. Meaning, there is 96% of values that have the wrong lepton asymmetry, and only 4% that have the correct value. This is a bit of an issue, because this means that the machine learning algorithm will be exposed to mostly wrong values and will learn them. But not so much the correct values. This migh impact the learning efficiency. Thankfully, since we have randomized the data, the WtoC ratio remains the same in the training and test data


def data_input_target_split(R):	# This function will be implementing the machine learning routine
	df_train, df_test, df = train_test_data_split(R)
	train_input, train_targets = df_train[['eps','K']].values, df_train['result binary'].values	# converts the dataframe into a numpy array! Useful to input this into scikit-learn.
	test_input, test_targets = df_test[['eps','K']].values, df_test['result binary'].values	# converts the dataframe into a numpy array! Useful to input this into scikit-learn.
	return train_input, train_targets, test_input, test_targets

def trainer(R):
	train_input, train_targets = data_input_target_split(R)[0],data_input_target_split(R)[1]
	print 'Split data into training and validation sets'
	X_train, X_valid, Y_train, Y_valid = cross_validation.train_test_split(train_input, train_targets, train_size = 0.9, random_state=10)
	print 'Defined the classifier'
	clf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=3, max_features='auto', bootstrap=True, oob_score=False, n_jobs=4, random_state=None, verbose=0, min_density=None, compute_importances=None)
	print 'Train the classifier'
	start = time.time()
	forest = clf.fit( X_train, Y_train )
	print 'Evaluate the classifier'
#	score = clf.score(X_valid, Y_valid)
	N = (len(Y_valid)-sum((np.array(clf.predict(X_valid))-np.array(Y_valid))**2))*len(Y_valid)**(-1)	#calculates the number of times the validation prediction matches the target value, as a percentage
	N = N*100
#	predictions = clf.predict(X_valid)
#	n = 0
#	for i in xrange(len(predictions)):
#		if predictions[i] == Y_valid[i]:
#			n = n+1
	print 'Calculate the RMSE score'
	RMSE = mean_squared_error(clf.predict(X_valid), Y_valid)**0.5 #This is the scoring we ought to use.
	end = time.time()
	T = end - start
#	print '- Finished training after %f seconds, with score = %f' % (T,score)
	print '- RMSE score = %f' % RMSE
	print '- Standard score = %f %%' %N
	return forest, RMSE, N

print trainer(1)[:]









